{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)\n",
    "\n",
    "This notebook provides an introduction to Recurrent Neural Networks (RNNs), covering their basics, advanced variants like LSTM and GRU, and applications in time-series prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "1. **Introduction to RNNs**\n",
    "2. **Basics of RNNs**\n",
    "3. **Long Short-Term Memory (LSTM)**\n",
    "4. **Gated Recurrent Units (GRU)**\n",
    "5. **Applications in Time-Series Prediction**\n",
    "6. **Example: Time-Series Prediction with LSTM**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction to RNNs\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed for sequential data. Unlike feedforward networks, RNNs have connections that form directed cycles, allowing them to maintain a 'memory' of previous inputs. This makes them suitable for tasks such as time-series prediction, natural language processing, and speech recognition.\n",
    "\n",
    "![RNN](https://miro.medium.com/v2/resize:fit:1400/1*WMnFSJHzOloFlJHU6fVN-g.gif)\n",
    "---\n",
    "\n",
    "## 2. Basics of RNNs\n",
    "\n",
    "### Mathematical Explanation\n",
    "An RNN processes a sequence of inputs $ x_1, x_2, \\dots, x_T $ and produces a sequence of hidden states $ h_1, h_2, \\dots, h_T $. The hidden state at time step $ t $ is computed as:\n",
    "\n",
    "$\n",
    "h_t = \\sigma(W_h h_{t-1} + W_x x_t + b_h)\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ h_t $ is the hidden state at time $ t $.\n",
    "- $ x_t $ is the input at time $ t $.\n",
    "- $ W_h $ is the weight matrix for the hidden state.\n",
    "- $ W_x $ is the weight matrix for the input.\n",
    "- $ b_h $ is the bias term.\n",
    "- $ \\sigma $ is the activation function (e.g., tanh or ReLU).\n",
    "\n",
    "![RNN Unrolled](https://blog.peddy.ai/assets/2019-05-26-Recurrent-Neural-Networks/rnn_rnn_unrolled.png)\n",
    "\n",
    "### Example: Simple RNN\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Define parameters\n",
    "input_size = 3\n",
    "hidden_size = 2\n",
    "sequence_length = 4\n",
    "\n",
    "# Initialize weights and biases\n",
    "W_h = np.random.randn(hidden_size, hidden_size)\n",
    "W_x = np.random.randn(hidden_size, input_size)\n",
    "b_h = np.random.randn(hidden_size, 1)\n",
    "\n",
    "# Input sequence\n",
    "X = [np.random.randn(input_size, 1) for _ in range(sequence_length)]\n",
    "\n",
    "# Initialize hidden state\n",
    "h_prev = np.zeros((hidden_size, 1))\n",
    "\n",
    "# RNN forward pass\n",
    "hidden_states = []\n",
    "for x in X:\n",
    "    h = np.tanh(np.dot(W_h, h_prev) + np.dot(W_x, x) + b_h)\n",
    "    hidden_states.append(h)\n",
    "    h_prev = h\n",
    "\n",
    "print(\"Hidden States:\\n\", hidden_states)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Long Short-Term Memory (LSTM)\n",
    "\n",
    "LSTMs are a special kind of RNN capable of learning long-term dependencies. They address the vanishing gradient problem by introducing gates that regulate the flow of information.\n",
    "\n",
    "![LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "\n",
    "### Mathematical Explanation\n",
    "An LSTM unit consists of:\n",
    "- **Forget Gate ($ f_t $):** Decides what information to discard from the cell state.\n",
    "- **Input Gate ($ i_t $):** Decides what new information to store in the cell state.\n",
    "- **Output Gate ($ o_t $):** Decides what to output based on the cell state.\n",
    "\n",
    "The equations for an LSTM unit are:\n",
    "\n",
    "$\n",
    "f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$\n",
    "\n",
    "$\n",
    "i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "$\n",
    "\n",
    "$\n",
    "\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "$\n",
    "\n",
    "$\n",
    "C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t\n",
    "$\n",
    "\n",
    "$\n",
    "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "$\n",
    "\n",
    "$\n",
    "h_t = o_t \\cdot \\tanh(C_t)\n",
    "$\n",
    "\n",
    "\n",
    "Reference:https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "### Example: LSTM in TensorFlow/Keras\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(sequence_length, input_size)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "print(model.summary())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Gated Recurrent Units (GRU)\n",
    "\n",
    "GRUs are a variation of LSTMs with a simplified architecture. They use update and reset gates to control the flow of information, making them computationally more efficient.\n",
    "\n",
    "### Mathematical Explanation\n",
    "A GRU unit consists of:\n",
    "- **Update Gate ($ z_t $):** Decides how much of the past information to keep.\n",
    "- **Reset Gate ($ r_t $):** Decides how much of the past information to forget.\n",
    "\n",
    "The equations for a GRU unit are:\n",
    "\n",
    "$\n",
    "z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "$\n",
    "$\n",
    "r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "$\n",
    "$\n",
    "\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\cdot h_{t-1}, x_t] + b_h)\n",
    "$\n",
    "$\n",
    "h_t = (1 - z_t) \\cdot h_{t-1} + z_t \\cdot \\tilde{h}_t\n",
    "$\n",
    "\n",
    "### Example: GRU in TensorFlow/Keras\n",
    "```python\n",
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "# Define GRU model\n",
    "model = Sequential([\n",
    "    GRU(50, activation='relu', input_shape=(sequence_length, input_size)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "print(model.summary())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Applications in Time-Series Prediction\n",
    "\n",
    "RNNs, LSTMs, and GRUs are widely used in time-series prediction tasks such as:\n",
    "- Stock price forecasting\n",
    "- Weather prediction\n",
    "- Energy consumption forecasting\n",
    "- Anomaly detection\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Example: Time-Series Prediction with LSTM\n",
    "\n",
    "Let's build an LSTM model to predict the next value in a synthetic time-series dataset.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Generate synthetic time-series data\n",
    "def generate_time_series(n_steps):\n",
    "    time = np.arange(0, n_steps)\n",
    "    data = np.sin(0.1 * time) + np.random.normal(0, 0.1, n_steps)\n",
    "    return data\n",
    "\n",
    "n_steps = 1000\n",
    "data = generate_time_series(n_steps)\n",
    "\n",
    "# Prepare dataset\n",
    "def prepare_dataset(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i+sequence_length])\n",
    "        y.append(data[i+sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sequence_length = 20\n",
    "X, y = prepare_dataset(data, sequence_length)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Split into training and testing sets\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(sequence_length, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Plot predictions vs actual values\n",
    "plt.plot(y_test, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "This notebook introduced the basics of RNNs, LSTMs, and GRUs, along with their applications in time-series prediction. We also implemented an LSTM model for time-series forecasting using TensorFlow/Keras.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "- Experiment with different RNN architectures and hyperparameters.\n",
    "- Explore advanced topics like attention mechanisms and transformer models.\n",
    "- Apply RNNs to real-world time-series datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
